from typing import Dict, List, Tuple, Union
import logging
import nltk
import sacrebleu
# from datatuner.lm.custom.utils import import_ser_calculator
from nltk.translate.meteor_score import meteor_score
from rouge_metric import PyRouge
from parent import parent

import sys
sys.path.append('/content/data2text_nlg')
import ser_calculator


# def import_ser_calculator():
#     #! this is terrible but there is no other easy way
#     import os
#     import sys
#     custom_dir_path = os.path.dirname(os.path.abspath(__file__))
#     ser_dir_path = os.path.join(custom_dir_path, "libs", "data2text-nlp")
#     sys.path.append(ser_dir_path)
#     import ser_calculator
#     return ser_calculator
    
# ser_calculator = import_ser_calculator()


def group_inputs_and_outputs_by_data(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> Tuple[List[List[str]], List[str]]:
    """Aggregate all the sentences which share the same original data.
    This is a slightly simplified data collection procedure of bleu(...) found in lm/metrics.py

    Args:
        inputs_and_outputs (List[Tuple[str, str, List[str]]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences

    Returns:
        Tuple(List[List[str]], List[str]): all the originals sentences and the predictions, grouped by data
    """
    grouped_items = {}
    for data, original_sentence, generated_sentences in model_inputs_and_outputs:
        generated_sentence = generated_sentences[0]
        #* lowercase sentences
        generated_sentence = generated_sentence.lower()
        original_sentence = original_sentence.lower()
        #* group the og and gen sentences by the values of their other keys (basically their data)
        if data in grouped_items:
            grouped_items[data]["original"].append(original_sentence)
            # this considerings just the last found sentence generated by certain data since the outputs doesn't change
            grouped_items[data]["prediction"] = generated_sentence
        else:
            grouped_items[data] = {"original": [original_sentence], "prediction": generated_sentence}
    return grouped_items


def extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items: Dict) -> Tuple[List[List[str]], List[str]]:
    """

    Args:
        grouped_items (Dict): _description_

    Returns:
        Tuple[List[List[str]], List[str]]: tuple of references with:
            - references: List of len as max_num_of_refs, where each element is another list of len as |hypotheses|,
                with either a reference or "" for each hypothesis
            - hypotheses: List of predictions/hypotheses/generated sentences
    """
    max_num_of_refs = max([len(entry["original"]) for entry in grouped_items.values()])
    hypotheses = []
    references = [[] for _ in range(max_num_of_refs)]
    for item in grouped_items.values():
        hypotheses.append(item["prediction"])
        for i in range(max_num_of_refs):
            try:
                references[i].append(item["original"][i])
            except:
                references[i].append("")
    return references, hypotheses


def extract_refs_and_hyps_from_grouped_items(
                                        grouped_items: Dict,
                                        pad_originals_length: bool = False,
                                        return_data: bool = False
                                    ) -> Union[Tuple[List[List[str]], List[str]], Tuple[List[List[str]], List[str], List[str]]]:
    """_summary_

    Args:
        grouped_items (Dict): _description_
        pad_originals_length (bool, optional): _description_. Defaults to False.
        return_data (boo, optional): returns data too (needed for PARENT score)

    Returns:
        Tuple[List[List[str]], List[str]]: tuple of references with:
            - references: List of len as |hypotheses|, where each element is another list with all the
                references for a given hypotesis. May be padded to make these lists all have the same size
                as the max num of references available for any given hypothesis.
            - hypotheses: List of predictions/hypotheses/generated sentences
    """
    max_num_of_refs = max([len(entry["original"]) for entry in grouped_items.values()])
    hypotheses = []
    references = []
    processed_data = []
    for data, item in grouped_items.items():
        processed_data.append(data)
        hypotheses.append(item["prediction"])
        if pad_originals_length and len(item["original"]) < max_num_of_refs:
            item["original"].extend(["" for _ in range(max_num_of_refs - len(item["original"]))])
        references.append(item["original"])
    outputs = references, hypotheses
    if return_data: outputs = references, hypotheses, processed_data
    return outputs


def corpus_level_bleu(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    """

    Args:
        model_inputs_and_outputs (List[Tuple[str, str, List[str]]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences

    Returns:
        float: corpus-level bleu
    """
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items)
    return sacrebleu.corpus_bleu(hypotheses, references).score


def corpus_level_chrf(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    """

    Args:
        model_inputs_and_outputs (List[List[str]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences

    Returns:
        float: corpus-level chrf
    """
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items)
    chrf = sacrebleu.metrics.CHRF()
    return chrf.corpus_score(hypotheses, references).score


def corpus_level_ter(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    """

    Args:
        model_inputs_and_outputs (List[List[str]]): list of a zip containing both the model's inputs and outputs,
            specifically: input data, original sentence, generated beam of sentences

    Returns:
        float: corpus-level ter
    """
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items_like_datatuner(grouped_items)
    ter = sacrebleu.metrics.TER()
    print(ter)
    return ter.corpus_score(hypotheses, references).score


def corpus_level_meteor(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items(grouped_items)
    scores = []
    for refs, hyp in zip(references, hypotheses):
        hyp = nltk.word_tokenize(hyp)
        refs = [nltk.word_tokenize(refs[0])]
        scores.append(meteor_score(refs, hyp))
    return sum(scores)*100/len(scores)


# def corpus_level_meteor(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
#     grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
#     references, hypotheses = extract_refs_and_hyps_from_grouped_items(grouped_items)
#     scores = []
#     scores = meteor_score(references, hypotheses)
#     return scores#sum(scores)*100/len(scores)

def corpus_level_rogue(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses = extract_refs_and_hyps_from_grouped_items(grouped_items)
    rouge = PyRouge(rouge_n=False, rouge_l=True)
    scores = rouge.evaluate(hypotheses, references)
    return scores["rouge-l"]["f"]*100


def corpus_level_parent(model_inputs_and_outputs: List[Tuple[str, str, List[str]]]) -> float:
    grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
    references, hypotheses, data = extract_refs_and_hyps_from_grouped_items(grouped_items, return_data=True)
    _, _, f_score = parent(
        hypotheses,
        references,
        data,
        avg_results=True,
    )
    return f_score


def corpus_level_ser(original_data: List[str], model_inputs_and_outputs: List[Tuple[str, str, List[str]]], dataset_name: str) -> Tuple[float]:
    """_summary_

    Args:
        original_data (List[str]): _description_
        model_inputs_and_outputs (List[Tuple[str, str, List[str]]]): _description_
        dataset_name (str): _description_

    Returns:
        Tuple[float]: SER, # wrong slots, UER, # wrong utterances/sentences
    """
    generated_sentences = [generated[0] for _, _, generated in model_inputs_and_outputs]
    return ser_calculator.calculate_ser(original_data, generated_sentences, dataset_name, output_uer=True)

# def calculate_ser(prediction, target):
#     prediction = nltk.word_tokenize(prediction[0])
#     target = nltk.word_tokenize(target)
#     N = len(target)  # Total number of slots in the reference
#     M = len(prediction)  # Total number of slots in the generated sentence/hypothesis
#     # Count the number of correctly realized slots (C)
#     C = sum(1 for slot in prediction if slot in target)
#     # Count the number of substitutions/value errors (S)
#     S = sum(1 for slot in prediction if slot not in target)
#     # Count the number of deletions/omissions (D)
#     D = sum(1 for slot in target if slot not in prediction)
#     # Count the number of insertions/hallucinations (I)
#     I = sum(1 for slot in prediction if slot not in target)
#     # Calculate precision (P) and recall (R)
#     P = C / M if M > 0 else 0
#     R = C / N if N > 0 else 0
#     # Calculate the SER score
#     SER = (S + D + I) / N if N > 0 else 0
#     return SER

# def corpus_level_ser(model_inputs_and_outputs):
#     grouped_items = group_inputs_and_outputs_by_data(model_inputs_and_outputs)
#     references, hypotheses = extract_refs_and_hyps_from_grouped_items(grouped_items)
#     scores = []
#     for refs, hyp in zip(references, hypotheses):
#         scores+=[calculate_ser(hyp, refs)]
#     return sum(scores)/len(scores)*100

# def create_metrics_compendium(
#                         model_inputs_and_outputs: List[Tuple[str, str, List[str]]],
#                         precomputed_ser: Tuple[float] = None,
#                         precomputed_bleu: float = None,
#                         original_data: List[str] = None,
#                         dataset_name: str = None
#                     ) -> Dict[str, float]:
#     """_summary_

#     Args:
#         model_inputs_and_outputs (List[Tuple[str, str, List[str]]]):
#         precomputed_bleu (float):
#         precomputed_ser (Tuple[float]): SER, # wrong slots, UER, # wrong utterances/sentences

#     Returns:
#         Dict:
#     """
#     if precomputed_ser is None and (original_data is None or dataset_name is None):
#         raise ValueError("Cannot add SER to metrics compendium without either the precomputed SER or the data needed to compute it.")
#     metrics_compendium = {}
#     #* check SER
#     if precomputed_ser is None:
#         precomputed_ser = corpus_level_ser(original_data, model_inputs_and_outputs, dataset_name)
#     metrics_compendium["ser"] = precomputed_ser[0]
#     metrics_compendium["wrong_slots"] = precomputed_ser[1]
#     metrics_compendium["uer"] = precomputed_ser[2]
#     metrics_compendium["wrong_utterances"] = precomputed_ser[3]
#     #* check bleu
#     if precomputed_bleu is None:
#         precomputed_bleu = corpus_level_bleu(model_inputs_and_outputs)
#     metrics_compendium["bleu"] = precomputed_bleu
#     #* check other metrics
#     rouge_score = corpus_level_rogue(model_inputs_and_outputs)
#     metrics_compendium["rouge"] = rouge_score
#     chrf_score = corpus_level_chrf(model_inputs_and_outputs)
#     metrics_compendium["chrf"] = chrf_score
#     ter_score = corpus_level_ter(model_inputs_and_outputs)
#     metrics_compendium["ter"] = ter_score
#     meteor_score = corpus_level_meteor(model_inputs_and_outputs)
#     metrics_compendium["meteor"] = meteor_score
#     # metrics_compendium["parent"] = corpus_level_parent(model_inputs_and_outputs)
#     return metrics_compendium